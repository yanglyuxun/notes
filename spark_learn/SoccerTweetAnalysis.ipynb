{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create a new SQLContext \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan, AFG', 'Albania, ALB', 'Algeria, ALG']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the country CSV file into an RDD.\n",
    "country_lines = sc.textFile('big-data-3/country-list.csv')\n",
    "country_lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Afghanistan', 'AFG'], ['Albania', 'ALB'], ['Algeria', 'ALG']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each line into a pair of words\n",
    "words = country_lines.map(lambda l:l.split(', '))\n",
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Afghanistan', 'AFG'), ('Albania', 'ALB'), ('Algeria', 'ALG')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each pair of words into a tuple\n",
    "tuples = words.map(tuple)\n",
    "tuples.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(country='Afghanistan', code='AFG'),\n",
       " Row(country='Albania', code='ALB'),\n",
       " Row(country='Algeria', code='ALG')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataFrame, look at schema and contents\n",
    "countryDF = sqlContext.createDataFrame(tuples, [\"country\", \"code\"])\n",
    "countryDF.printSchema()\n",
    "countryDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|          tweet_id|in_reply_to_status_id|in_reply_to_user_id|retweeted_status_id|retweeted_status_user_id|           timestamp|              source|                text|expanded_urls|\n",
      "+------------------+---------------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|322185112684994561|   322137634161971200|          222854927|               null|                    null|2013-04-11 03:11:...|\"<a href=\"\"http:/...|@Bill_Porter nice...|         null|\n",
      "|321279208158552064|   321275690811011072|           15105039|               null|                    null|2013-04-08 15:11:...|\"<a href=\"\"http:/...|@sudhamshu after ...|         null|\n",
      "|321155708324311040|   321148055212679168|           52959201|               null|                    null|2013-04-08 07:01:...|\"<a href=\"\"http:/...|@neetashankar Yea...|         null|\n",
      "+------------------+---------------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id: integer (nullable = true)\n",
      " |-- retweeted_status_id: long (nullable = true)\n",
      " |-- retweeted_status_user_id: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- expanded_urls: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read tweets CSV file into RDD of lines\n",
    "tt = sqlContext.read.csv('big-data-3/tweets.csv',\n",
    "                        header='true',\n",
    "                        inferSchema='true')\n",
    "tt.show(3)\n",
    "tt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563\n",
      "3545\n",
      "+------------------+---------------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|          tweet_id|in_reply_to_status_id|in_reply_to_user_id|retweeted_status_id|retweeted_status_user_id|           timestamp|              source|                text|expanded_urls|\n",
      "+------------------+---------------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|322185112684994561|   322137634161971200|          222854927|               null|                    null|2013-04-11 03:11:...|\"<a href=\"\"http:/...|@Bill_Porter nice...|         null|\n",
      "+------------------+---------------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the data: some tweets are empty. Remove the empty tweets using filter() \n",
    "print(tt.count())\n",
    "tt2 = tt.na.drop(subset='text')\n",
    "print(tt2.count())\n",
    "tt2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|@Bill_Porter nice...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('@Bill_Porter', 1), ('nice', 46), ('to', 1744), ('know', 161), ('that', 304)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform WordCount on the cleaned tweet texts. (note: this is several lines.)\n",
    "texts = tt2.select('text')\n",
    "texts.show(1)\n",
    "words2 = texts.rdd.flatMap(lambda l:l[0].split())\n",
    "counts = words2.map(lambda w:(w,1))\n",
    "word_counts = counts.reduceByKey(lambda a,b:a+b)\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|@Bill_Porter|    1|\n",
      "|        nice|   46|\n",
      "+------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame of tweet word counts\n",
    "wcDF = sqlContext.createDataFrame(word_counts, [\"word\", \"count\"])\n",
    "wcDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----+\n",
      "|  country|code|     word|count|\n",
      "+---------+----+---------+-----+\n",
      "| Malaysia| MAS| Malaysia|    7|\n",
      "|    India| IND|    India|   27|\n",
      "|   Norway| NOR|   Norway|    1|\n",
      "|   Bhutan| BHU|   Bhutan|   13|\n",
      "|Indonesia| IDN|Indonesia|    2|\n",
      "|Australia| AUS|Australia|    2|\n",
      "| Pakistan| PAK| Pakistan|    1|\n",
      "+---------+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the country and tweet data frames (on the appropriate column)\n",
    "dfj = countryDF.join(wcDF,countryDF.country==wcDF.word )\n",
    "dfj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1: number of distinct countries mentioned\n",
    "dfj.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        53|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 2: number of countries mentioned in tweets.\n",
    "from pyspark.sql.functions import sum\n",
    "dfj.agg(sum('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------+-----+\n",
      "| country|code|    word|count|\n",
      "+--------+----+--------+-----+\n",
      "|   India| IND|   India|   27|\n",
      "|  Bhutan| BHU|  Bhutan|   13|\n",
      "|Malaysia| MAS|Malaysia|    7|\n",
      "+--------+----+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 1: top three countries and their counts.\n",
    "from pyspark.sql.functions import desc\n",
    "dfj.sort(desc('count')).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+-----+\n",
      "|country|code|word|count|\n",
      "+-------+----+----+-----+\n",
      "+-------+----+----+-----+\n",
      "\n",
      "+-------+----+----+-----+\n",
      "|country|code|word|count|\n",
      "+-------+----+----+-----+\n",
      "+-------+----+----+-----+\n",
      "\n",
      "+-------+----+----+-----+\n",
      "|country|code|word|count|\n",
      "+-------+----+----+-----+\n",
      "+-------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 2: counts for Wales, Iceland, and Japan.\n",
    "dfj.filter(dfj['country'] == 'Wales').show()\n",
    "dfj.filter(dfj['country'] == 'Iceland').show()\n",
    "dfj.filter(dfj['country'] == 'Japan').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
